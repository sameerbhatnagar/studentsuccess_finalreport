--- 
title: "Data Mining for Student Success and Perseverance"
author:
 - Sameer Bhatnagar
 - Jonathan Guillemette
 - Micheal Dugdale
 - Sahir Bhatnagar
 - Nathaniel Lasry
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib,studentsuccess.bib]
biblio-style: apalike
link-citations: true
github-repo: sbhatnagar/studentsuccess_finalreport
description: "This is the Final Report for PAREA grant"
---

# Preface

This report summarizes the work done by our team on using college registration records at three diffrenet anglophone CEGEPS in Montreal in order to find predictors of attrition.


<!--chapter:end:index.Rmd-->

# Introduction {#intro}

## Context
This report outlines the results from a three year intercollegiate research project funded by the**PAREA** agency ( _Programmme d'Aide à la Recherche en Enseignment et Apprentissage_) from the _Ministère de l'Education_ of the provincial Government of Quebec.

In the province of Quebec, students finish their secondary education at what is the equivalent of Grade 11 in other parts of North America. Students are then able to attend **CEGEP** ( _Collège d'enseignment général et professionel_) for either 

 - two years, as part of pre-university program, e.g Science, Social Science, Liberal Arts
 - three years, as part of technical program, meant specifically to lead directly to the job market, e.g. Nursing, Civil Engineering Technology, Diagnotsic Imaging Technology
 
There are 48 CEGEPs in the Quebec network, and public or private, they all fall under the purview of the _Ministère de l'Education et Enseignment Superieur_. Over the past twenty years, there has been significant work[@jorgensen2003students; @jorgensen2005academic; @jorgensen2009predicting; @riviere1995decrocheurs; @shaienks2008statcan] and media [@breton2016soleil; @dion-viens2015lapresse; @duchaine2017lapresse] on the topic of student attrition in CEGEP. The scholarly work done has often focused on determing predictors of attrition through surveys, or focused on specific vulnerable sub populations. The media has often reported on government figures, which rely on data that looks at information at a very coarse level of granularity (of students graduated from high schoolhow many obtain diplomas from CEGEP)

## Objectives

Almost all of the CEGEP's use the same database system, known as **CLARA** (developped by the company Skytech) in order to manage the data related to student admission, registration and graduation. Our research team's main objective is to leverage this uniformity of how data is automatically generated and stored, in order to determine if, in this wealth of data, there might be predictors of student attrition. This effort stands apart from previous work and reports in that:

 - the data analyzed is much finer-grained: the unit of analysis is down to the semester registration records for each student
 - we look at the general population of students
 - to our knowledge, this is the first ever such study to span multiple CEGEPs, which we hope adds a greater relibility and validity to our findings.
 
This project has two specific objectives:

 1) find predictors of students dropping out, whether they be demographic, or based in academic performance, on a term by term basis. 
 2) evaluate methods by which students can be automatically flagged as being at risk of dropping out, without so much a focus on understanding why, but for the purpose of "general offers of support" or further investigation

This report is structured to reflect these two objectives. Namely, we begin with
 - a standard review of previous [related work](#littreview),
 - a section on [descriptive statistics](#descriptive) which gives an overview of the dataset.
 
We then move on to our methods and models over two chapters: 
 - the [first](#explanatory) addresses objective 1, outlining our efforts to find explanatory predictors of student attrition in classical statistical models. 
 - The [following chapter](#predictive) addresses objective 2, describing modern methods from the field of machine learning, which, at the expense of model interpretability, are fit to provide maximum predictive accuracy in identifying students at-risk. 
 
The report then concludes with
 - [comparisons](#comparisons) of these methods with each other, and to current intervention frameworks in place at participating colleges
 - an auxilliary chapter looking at students from the division of [Continuing Education](#conted) 
 - a [concluding chapter](#conclusion), with reccomendations for future directions



<!--chapter:end:01-intro.Rmd-->

# Literature {#littreview}

**Under Construction **

## Modeling success and attrition in CEGEP
The most important relevant work for this project is [[@jorgensen2009pareafinalreport]].


## Predictive Modelling in Learning Analytics and Educational Data Mining
[@hla2017]

<!--chapter:end:02-literature.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---

# Descriptive Statistics {#descriptive}

** Under Construction **

```{r descriptive-local-setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE,message = FALSE)
knitr::read_chunk("bin/03-descriptive-stats-code.R")
```

```{r packages}
```

Here in we will describe
 - the data set
 - the methods by which we label students at risk
 - the distributions of at-risk students by 
     - demographic indicators
     - registration record indicators 

## Demographics across the colleges and major programs

### Dawson
```{r demographics-dawson}
```

### John Abbott College
```{r demographics-jac}
```

### Vanier
```{r demographics-vanier}
```

## Academic Performance in Semesters leading to drop-out
```{r descriptive-stats-setup}
```
### Grades 
Do students who drop out do so because of poor grades? What fraction of students are counted year after year as drop-outs and labeled as problems to be solved by the system while being exemplary students in terms of academic performance. Armed with this dataset, we can get the answer to that question. Let us begin by looking at the average grades of students who eventually dropped out compared to grades of students who haven't. The following set of graphs will look at that comparison for 3 different semesters: the semester in which they dropped out (or graduated), the semester before that and the one before that. Let us see what the data says.
<<<<<<< HEAD

```{r Average comp}
```

```{r}
x<-students_last_session$Average[which(students_last_session$status=="out")]
```
From the average total grade between the drop outs and the graduates, we can clearly see that the distributions are significantly different, but what is surprising is that `r round(sum(x>75, na.rm=T)/length(x),digits=2)*100`% of students have an average grade above 75% and that more than `r round(sum(x>59, na.rm=T)/length(x),digits=2)*100`% of the students who dropped out had an average grade above 60%. In other words, most students who drop out have passing averages. One hypothesis for this effect is that students who end up dropping out start with good semesters and have their performance decline closer to the final semester. Let us verify this hypothesis. 

Let us now look at the performance of drop outs vs graduates on a semester by semester basis. Let us start by looking at the average grades of students in their last semester during which they are either graduating or dropping-out. 
```{r Average-comp}
```

```{r Average-comp-ttest}
```

```{r}
x<-students_last_session$Average_all_courses[which(students_last_session$status=="out")]
```
From the average total grade between the drop outs and the graduates, we can clearly see that the distributions are significantly different, but what is surprising is that `r round(sum(x>75, na.rm=T)/length(x),digits=2)*100`% of students have an average grade above 75% and that more than `r round(sum(x>59, na.rm=T)/length(x),digits=2)*100`% of the students who dropped out had an average grade above 60%. In other words, most students who drop out have passing averages. One hypothesis for this effect is that students who end up dropping out start with good semesters and have their performance decline closer to the final semester. Let us verify this hypothesis. 

Let us now look at the performance of drop outs vs graduates on a semester by semester basis. Let us start by looking at the average grades of students in their last semester during which they are either graduating or dropping-out.  

```{r Last-semester-comp}
```

```{r Last-semester-comp-ttest}
```

```{r}
x<-avg_grade_per_term[,.SD[.N],by=c("student_number")][status=="out"]$avg_grade
```

In this data, we can clearly observe a stark difference between the graduates and the drop outs. First of all, note that `r round(sum(x>75, na.rm=T)/length(x),digits=2)*100`% of students still have a term average over 75% in the semester in which they drop out. To push it further `r round(sum(x>80, na.rm=T)/length(x),digits=2)*100`% of students who drop out have an average grade over 80%. The data clearly suggests that some of the drop outs aren't dropping out because of academic performance. Furthermore, the long tail of the data on the low end of performance suggests that some of the students stopped coming to class prior to the end of the semester resulting in very low grades that serve to drive their cegep average grades from the graph above even lower. `r round(sum(x<40, na.rm=T)/length(x),digits=2)*100`% of students have a grade below 40 suggesting that they have indeed stopped coming to school some time during the semester. What if these students hadn't stopped coming, would their performance be similar to that of graduates? 

Let us now turn our attention to the semester before the one where they graduate or drop out.

```{r N-1-semester-comp}
```
<<<<<<< HEAD
```{r}
x<-c2[,.SD[.N],by=c("student_number")][status=="out"]$V1
```

In this data, we can clearly observe a stark difference between the graduates and the drop outs. First of all, note that `r round(sum(x>75, na.rm=T)/length(x),digits=2)*100`% of students still have a term average over 75% in the semester in which they drop out. To push it further `r round(sum(x>80, na.rm=T)/length(x),digits=2)*100`% of students who drop out have an average grade over 80%. The data clearly suggests that some of the drop outs aren't dropping out because of academic performance. Furthermore, the long tail of the data on the low end of performance suggests that some of the students stopped coming to class prior to the end of the semester resulting in very low grades that serve to drive their cegep average grades from the graph above even lower. `r round(sum(x<40, na.rm=T)/length(x),digits=2)*100`% of students have a grade below 40 suggesting that they have indeed stopped coming to school some time during the semester. What if these students hadn't stopped coming, would their performance be similar to that of graduates? 

Let us now turn our attention to the semester before the one where they graduate or drop out. 
```{r N-1 semester comp}
```
```{r}
x<-c2[,.SD[.N-1],by=c("student_number")][status=="out"]$V1
```


```{r N-1-semester-comp-ttest}
```

```{r}
x<-avg_grade_last_term_minus1[status=="out"]$avg_grade
```
>>>>>>> 09d5dad94bfceabd1acdaa6e46b0bbf82e89c757
The data clearly shows that even if there are significant differences between the groups, the drop out student population is getting closer to the graduate population. A section of `r round(sum(x>75, na.rm=T)/length(x),digits=2)*100`% is observed to have an average grade above 75%.

Finally, let us look at 2 semesters before they graduate or drop-out. We are again expecting the same trend.

```{r N-2-semester-comp}
```

```{r N-2-semester-comp-ttest}
```
```{r}
x<-c2[,.SD[.N-2],by=c("student_number")][status=="out"]$V1
```
## Conclusion

In conclusion, the data strongly suggests that there are approximately 20% of students who consistently have averages above 75%, but still drop out. Therefore, that section of the drop out population is a section that will always go undetected if only traditional academic failure metrics are used to assess which students are likely to drop out. Students who move out to the US or the rest of Canada after their second semester and those who drop out by lack of interest are two potential student types that will always drop out no matter what remedial solutions are offered for them. 

One of the profile level Key Performance Indicators that colleges are supposed to specifically keep track of is third semester retention. In this light, we can somewhat flip the line of questionning above, and look to see if the distribution of grades in the third semester students looks different for those who will drop out, and those who will continue on. 

```{r third-semester-attrition-by-program}
```


In conclusion, the data strongly suggests that there are approximately 20% of students who consistently have averages above 75%, but still drop out. Therefore, that section of the drop out population is a section that will always go undetected if only traditional academic failure metrics are used to assess which students are likely to drop out. Students who move out to the US or the rest of Canada after their second semester and those who drop out by lack of interest are two potential student types that will always drop out no matter what remedial solutions are offered for them. 

### Passed vs. Failed Courses
The line of questionning above can be repeated, but instead of looking at the average grade of courses taken in a term, we can instead look to see if the number of courses passed, or the proportion failed, might be different for students who drop out versus those who do not. 

```{r failed-courses}
```

What we remark in the table above is that the number of courses failed does not seem to differentiate students who will drop out after their third term. But perhaps, we should consider the fraction of courses that a student took in their third term, and failed?

```{r prop-failed-courses}
```

For the above graphic, we calculate, for each student in their term, the fraction of courses that they took and _failed_, and we plot the distributions for the group of students we know who dropped out, and those we know stayed in the college for a fourth term. The red boxplot shows that,of the students who dropped out after their third term, 50% of them failed more than than half of their classes in that third term (the central notch represents a 95% confidence interval on the median). Conversely, the distribution on the left, squashed down at 0, shows that almost all students who stay on for a fourth term, pass all of their third term classes (The additional dots shown in this distribution are considered outliers to the distribution, meaning there are some students who failed all of their third term classes, and decided to stay for an additional fourth term).



<!--chapter:end:03-descriptive-stats.Rmd-->

# Methods Centered on Determining Predictive Factors {#explanatory}

**Under Construction**

This chapter will be focused on methods which have a sound probabilistic framework, and allow for inference into the statistical importance of predictive factors.

 - Logistic Regression
 - Mixed Effects Models

<!--chapter:end:04-methods-inference.Rmd-->

# Methods centered on predicting at-risk students {#predictive}

**Under construction**

Over the last twenty years, there has an increasing amount of work in the applied social sciences that explore the use of what [@breiman2001twocultures] refers to as "algorithmic modelling"", as opposed to "data modelling". He describes these as two cultures, the former being made up of mostly computer scientists, and the latter being made up statisticians (the methods therein are the ones explored in the previous chapter of this report). The key metric in such classical methods are __goodness of fit__, and such "explanatory"" modelling aims to find associative and causal relationships between predictors. Meanwhile, "algorithmic"", or "predictive"" methods emphasize determining any function that maps input variables to output responses, with less regard for a probabilitic framework that allows for causation, focusing solely on emprirical precision[@shmueli2010explain]. In the recently published **Handbok of Learning Analytics**[@hla2017], published by the Society of Learning Analytics Research, [@bergner_measurement_2017] asserts that the researchers looking into educational data stand to gain from understanding the nuances of both methodologies, as previous work has shown the strengths and weakeness of either in this domain.

The previous chapter explored how classical statistical models can be built and used to determine what are the factors that influence dropout. This is useful for policy makers and admninstrators who want to dedicate resources in the most strategic places. However this chapter will explore models whose inner workings are less interpretable, but whose primary objective is prediction/identification of at-risk students. This is useful in the context where college administration has some blanket intervention that it would like to apply, and we just want to ensure that the students most in need are reached. Despite the less clear interpratability of factors in these _predictive_ models (as compared to the _explanatory_ models in the previous chapter), we will stil explore methods to "open up the black box", and determine which features are most important in achieving both accrate and sensitive prediction.

## Decision Trees and Random Forests

## Neural Networks

<!--chapter:end:05-methods-prediction.Rmd-->

# Comparisons {#comparisons}

** Under Construction **

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
knitr::read_chunk('bin/06-comparisons-code.R')
```

```{r comparisons-setup}
```

This chapter will compare the effectiveness of the methods developped in the previous two chapters, and compare them to more basic approaches to identifying students at-risk.

For example, we know that some CEGEPs have implemented a policy whereby they identify students as being at risk based on their mid-term assessments: if the student receives a certain number of "at-risk" or "failing" results, they are automatically sent an email referring them to academic support services. 

Based on this, we can ask the following research questions : 
 - how effective is this approach at identifying students who drop-out? 
 - how does this approach compare to our models from the previous chapters?

We begin with a basic logistic regression with demographic variables, and as well as the number of each type of results of mid-term assessment, for students in their last term at the college. With these predictors, we try to predict if students are about to graduate, or simply not register again.
```{r model-last-term-status}
```


<!--chapter:end:06-comparisons.Rmd-->

# Continuing Education at Dawson {#conted}

** Under Construction **

```{r conted-local-setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE,message = FALSE)
knitr::read_chunk("bin/ContEdDawson.R")
```

```{r conted-setup}
```

The department of Continuing at Education offers evening courses across many disciplines throughout the year. The purpose of this report is to give a broad overview of the demographics and sector-level metrics for students taking courses within Cont Ed.

## Services

Herein, we look at the services offered by the Division of Continuing Education. This section is meant to give an overview of the breadth and depth of the offerings outside of the regular day division.

### Continuing Education's Growth over Time

How have each of the departments increased their ContEd offerings over time? 

```{r dept-level-sizes-over-time}
```

The above figure shows that the demand for courses in the Math department (department 201) has steadily increased over the last seven years, as have the Humanities (department 345), and English (602 and 603).

### Department Level Seat Distributions

Which parts of Continuing Education are the most important in 2016? 

```{r dept-level-sizes-2016}
```

The previous plot shows that while there are many departments that offered courses in 2016, the bulk share were in Math, English and the Huamnities. Of note here are the departments that require specialized lab spaces, such as Chemistry (202) and Physics (203), which will be looked at more closely as we move forward.


### Course Level distributions

If we look at the three most important departments (Math,Humanities and English) in 2016, how are the seats distrinuted across courses?

```{r big-dept-seat-dist-by-course}
```


### Specialized spaces

If we look at the departments that require specialized spaces (i.e. labs), in 2016, how are the seats distributed by course? 
 - Has this evolved over time?
 - What are the seasonal variations?

#### Winter
```{r lab-dept-seat-dist-by-course-winter}
```

#### Fall
```{r lab-dept-seat-dist-by-course-fall}
```

#### Summer
```{r lab-dept-seat-dist-by-course-summer}
```



## Students

### Demographics
Who are the students using continuing education services?

  - are there gender differences?
  - are there age differences?
  - do these demographics change over time?
  - are there seasonal variations?
  - are there differences in different departments?

#### Sexe
```{r Sexe-demographics-over-time}
```

Of note in the above figure:
  - English (603): 2016 marks the first year where male students have a passing average
  - Math (201): failing averages in all semesters
  - Chemistry (202): fairly consistent performance across time for both genders, with slightly aging demographic over the years
  - Physics (203): most consistent in academic performance and demographics
  - Computer Science (420): most widely varying in both dimensions across time
  
What is impact of condensed summer term? 

```{r Sexe-demographics-over-time-summer}
```

Finally, as there seems to be relatively little change in these patterns over time, we can collapse all over the past seven years, 

```{r Sexe-demographics}
```

and then add on the average grade achieved by students with the same gender, in the same courses from the same disciplines, but from the ``regular'' day division.

```{r Sexe-demographics-day-mean}
```

What is clear from the above is that 
 - in general, students do better in their day division courses than in the their Continuing Education Counterparts
 - Math is the most difficult subject both in the day division and in the evening
 - the Physics department has the smallest performance gap between day students and those in Continuing Education

#### Birth Place
Now, instead of looking at effect of gender, we focus instead on Birth Place. To Quebec residents stand out in any consistent way, as compared to those born elsewhere?

```{r birth-place-demographics-over-time}
```

Some observations from the above figure:
 - The older students in Continuing Education were in large part not born in Quebec. 

Again, what is impact of condensed summer term? 

```{r birth-place-demographics-over-time-summer}
```

Finally, as there seems to be relatively little change in these patterns over time, we can collapse all over the past seven years, 

```{r birth-place-demographics}
```

and then add on the average grade achieved by students with the same **birth place** (namely quebec residents vs *other*), in the same courses from the same disciplines, but from the ``regular'' day division.

```{r birth-place-demographics-day-mean}
```

The same observation as from our study of gender demographics above holds: regardless of birthplace, day division students outperform their day division counterparts, and this gap is smallest in the physics department. The overall weakness in math is not limited to any one gender, nor explained by birth place

#### Mother Tongue
Finally we look at the possible impact of Mother Tongue. How do anglophones, francophones, and allophones compare in Continuing Education?

```{r langue-demographics-over-time}
```

Again, what is impact of condensed summer term? 

```{r langue-demographics-over-time-summer}
```

Finally, as there seems to be relatively little change in these patterns over time, we can collapse all over the past seven years, 

```{r langue-demographics}
```

and then add on the average grade achieved by students with the same **mother tongue**, in the same courses from the same disciplines, but from the ``regular'' day division.

```{r langue-demographics-day-mean}
```




### Success Rates

There are many students who take courses through both Continuing Education, and the regular day division over the course of their time at Dawson. One way of measuring the difference in the student experience here would be to look at those students alone, and compare the average of their grades in each division, while controlling for discipline. (the diagonal in each facet is simply meant to provide a reference for a hypothetical 1-to-1 correlation.) In order to control for student strength, we bin by overall average of high school grades. 

```{r day-conted-same-students-AN, fig.show='hold'}
```

```{r day-conted-same-students-FR, fig.show='hold'}
```

```{r day-conted-same-students-AU}
```

The above graphic shows that stronger students seem to be able to do as well in their day courses as they do in their evening ones, but this is not the cas for weaker students. What's more, there seems to be no consistency across departments.

The following graphic is the same as above, but instead of binning students by their High School Average (as a proxy for over student strength), we use students first term CEGEP grades, and then plot their performance in day and evening courses averaged over all subsequent terms (disaggregated again by discipline).

```{r day-conted-same-students-term1vsall-AN, fig.show='hold'}
```

```{r day-conted-same-students-term1vsall-FR, fig.show='hold'}
```

```{r day-conted-same-students-term1vsall-AU}
```


<!--chapter:end:07-contedDawson.Rmd-->

# Conclusion {#conclusion}

** Under Construction **

<!--chapter:end:08-summary.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`

<!--chapter:end:09-references.Rmd-->

